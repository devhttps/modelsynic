# ModelSync vLLM Integration

## üöÄ **Integraci√≥n Completa de vLLM con ModelSync**

Esta integraci√≥n permite usar vLLM (Very Large Language Models) con versionado autom√°tico en ModelSync, proporcionando un sistema completo para experimentos con modelos de lenguaje.

## üìã **Caracter√≠sticas Principales**

### ‚úÖ **API de vLLM Integrada**
- **Servicio FastAPI** dedicado para vLLM
- **Integraci√≥n autom√°tica** con ModelSync
- **Versionado de generaciones** autom√°tico
- **Auditor√≠a completa** de todas las operaciones
- **M√©tricas en tiempo real** del servicio

### ‚úÖ **Gesti√≥n de Modelos LLM**
- **Carga din√°mica** de modelos
- **Soporte para m√∫ltiples modelos** simult√°neos
- **Versionado de modelos** en ModelSync
- **Metadatos completos** (par√°metros, m√©tricas, uso)

### ‚úÖ **Experimentaci√≥n Avanzada**
- **Experimentos controlados** con diferentes par√°metros
- **Comparaci√≥n autom√°tica** de configuraciones
- **Generaci√≥n en lote** optimizada
- **An√°lisis de rendimiento** detallado

### ‚úÖ **CLI Integrado**
- **Comandos espec√≠ficos** para vLLM
- **Gesti√≥n desde terminal** completa
- **Monitoreo en tiempo real** del servicio

## üõ†Ô∏è **Instalaci√≥n y Configuraci√≥n**

### **1. Instalar Dependencias**
```bash
pip install -r requirements.txt
```

### **2. Iniciar Servicio vLLM**
```bash
# Opci√≥n 1: CLI
modelsync llm start

# Opci√≥n 2: Python directo
python -m modelsync.llm.vllm_api

# Opci√≥n 3: Docker
docker-compose -f docker-compose.vllm.yml up
```

### **3. Verificar Estado**
```bash
modelsync llm_status
```

## üìö **Uso de la API**

### **Endpoints Principales**

| Endpoint | M√©todo | Descripci√≥n |
|----------|--------|-------------|
| `/` | GET | Informaci√≥n de la API |
| `/health` | GET | Estado del servicio |
| `/models` | GET | Listar modelos cargados |
| `/generate` | POST | Generar texto individual |
| `/generate/batch` | POST | Generar texto en lote |
| `/modelsync/status` | GET | Estado de ModelSync |
| `/modelsync/init` | POST | Inicializar ModelSync |
| `/metrics` | GET | M√©tricas del servicio |

### **Ejemplo de Generaci√≥n Individual**
```python
from modelsync.llm.vllm_client import VLLMClient

client = VLLMClient()

response = client.generate(
    prompt="Hola, ¬øc√≥mo est√°s?",
    max_tokens=100,
    temperature=0.7,
    save_to_version_control=True
)

print(response['text'])
print(f"ModelSync ID: {response['model_version_id']}")
```

### **Ejemplo de Generaci√≥n en Lote**
```python
prompts = [
    "Explica qu√© es la inteligencia artificial",
    "Cu√©ntame un chiste",
    "¬øCu√°l es la capital de Francia?"
]

responses = client.generate_batch(
    prompts=prompts,
    max_tokens=50,
    temperature=0.8
)

for i, response in enumerate(responses):
    print(f"{i+1}. {response['text']}")
```

## üß™ **Experimentaci√≥n Avanzada**

### **Gestor de Experimentos**
```python
from modelsync.llm.vllm_client import VLLMExperimentManager

experiment_manager = VLLMExperimentManager(client)

# Ejecutar experimento
experiment_result = experiment_manager.run_experiment(
    experiment_name="temperature_test",
    prompts=["Escribe un poema sobre la tecnolog√≠a"],
    parameters={
        "max_tokens": 100,
        "temperature": 0.9,
        "top_p": 0.95
    },
    description="Prueba de creatividad con alta temperatura"
)

print(f"Exitosos: {experiment_result['successful_generations']}")
```

### **Comparaci√≥n de Par√°metros**
```python
# Comparar diferentes temperaturas
comparison_result = experiment_manager.compare_parameters(
    base_prompts=["Escribe una historia corta"],
    parameter_sets=[
        {"temperature": 0.3, "max_tokens": 50},  # Conservador
        {"temperature": 0.7, "max_tokens": 50},  # Balanceado
        {"temperature": 1.2, "max_tokens": 50}   # Creativo
    ],
    experiment_name="temperature_comparison"
)
```

## üñ•Ô∏è **Comandos CLI**

### **Comandos B√°sicos**
```bash
# Iniciar servicio vLLM
modelsync llm start

# Verificar estado
modelsync llm_status

# Generar texto
modelsync llm generate --prompt "Hola, ¬øc√≥mo est√°s?" --max-tokens 100

# Ver ayuda
modelsync llm --help
```

### **Par√°metros de Generaci√≥n**
```bash
modelsync llm generate \
  --prompt "Escribe un poema" \
  --max-tokens 200 \
  --temperature 0.8 \
  --model "meta-llama/Meta-Llama-3-8B-Instruct"
```

## üê≥ **Docker y Contenedores**

### **Docker Compose para vLLM**
```bash
# Iniciar todos los servicios
docker-compose -f docker-compose.vllm.yml up

# Solo vLLM API
docker-compose -f docker-compose.vllm.yml up modelsync-vllm

# En segundo plano
docker-compose -f docker-compose.vllm.yml up -d
```

### **Dockerfile Espec√≠fico**
```dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu20.04
# ... configuraci√≥n espec√≠fica para vLLM
```

## üìä **Monitoreo y M√©tricas**

### **M√©tricas Disponibles**
- **Modelos cargados** y su estado
- **Total de requests** procesados
- **Tiempo promedio** de respuesta
- **Uso de memoria** y GPU
- **Actividad reciente** del servicio
- **Integraci√≥n ModelSync** status

### **Auditor√≠a Completa**
- **Log de todas las generaciones** con metadatos
- **Rastreo de usuarios** y acciones
- **Historial de experimentos** detallado
- **M√©tricas de rendimiento** por modelo

## üîß **Configuraci√≥n Avanzada**

### **Variables de Entorno**
```bash
export CUDA_VISIBLE_DEVICES=0
export VLLM_USE_MODELSCOPE=False
export PYTHONPATH=/app
```

### **Configuraci√≥n de Modelos**
```python
# Cargar modelo personalizado
client.load_model("microsoft/DialoGPT-medium")

# Verificar modelos disponibles
models = client.list_models()
```

## üöÄ **Ejemplos Pr√°cticos**

### **1. Ejemplo B√°sico**
```bash
# Ejecutar ejemplo completo
python examples/vllm_example.py
```

### **2. Integraci√≥n con ModelSync**
```python
# El versionado es autom√°tico
response = client.generate(
    prompt="Tu prompt aqu√≠",
    save_to_version_control=True  # Por defecto True
)

# Ver en ModelSync
modelsync model list
modelsync log
```

### **3. Experimentos de Investigaci√≥n**
```python
# Configurar experimento cient√≠fico
experiment_config = {
    "prompts": ["Prompt base para comparar"],
    "parameter_sets": [
        {"temperature": 0.1, "top_p": 0.9},
        {"temperature": 0.5, "top_p": 0.9},
        {"temperature": 1.0, "top_p": 0.9}
    ],
    "repetitions": 3
}

# Ejecutar y analizar
results = run_scientific_experiment(experiment_config)
```

## üîç **Troubleshooting**

### **Problemas Comunes**

**1. Servicio no responde**
```bash
# Verificar estado
modelsync llm_status

# Reiniciar servicio
modelsync llm start
```

**2. Modelo no carga**
```bash
# Verificar GPU disponible
nvidia-smi

# Verificar memoria
free -h
```

**3. Error de ModelSync**
```bash
# Inicializar ModelSync
curl -X POST http://localhost:8001/modelsync/init
```

## üìà **Rendimiento y Optimizaci√≥n**

### **Recomendaciones**
- **GPU con al menos 8GB** de VRAM para modelos grandes
- **SSD r√°pido** para carga de modelos
- **Memoria RAM suficiente** (16GB+ recomendado)
- **Configurar CUDA** correctamente

### **Monitoreo de Recursos**
```bash
# Ver uso de GPU
nvidia-smi

# Ver m√©tricas del servicio
curl http://localhost:8001/metrics
```

## üéØ **Casos de Uso**

### **1. Investigaci√≥n en IA**
- Experimentos controlados con diferentes par√°metros
- Comparaci√≥n sistem√°tica de modelos
- An√°lisis de rendimiento detallado

### **2. Desarrollo de Aplicaciones**
- Prototipado r√°pido de funcionalidades LLM
- Testing automatizado de generaci√≥n
- Versionado de configuraciones

### **3. Producci√≥n**
- API escalable para inferencia
- Monitoreo en tiempo real
- Auditor√≠a completa de uso

## üéâ **Conclusi√≥n**

La integraci√≥n de vLLM con ModelSync proporciona una soluci√≥n completa para:

- **Experimentaci√≥n** con modelos de lenguaje
- **Versionado** autom√°tico de generaciones
- **Colaboraci√≥n** en equipos de investigaci√≥n
- **Producci√≥n** escalable y monitoreada

¬°Comienza a experimentar con modelos de lenguaje de manera profesional y versionada! üöÄ
