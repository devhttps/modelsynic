#!/usr/bin/env python3
"""
Ejemplo de uso de vLLM con ModelSync
"""

import os
import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modelsync.llm.vllm_client import VLLMClient, VLLMExperimentManager

def demonstrate_vllm_integration():
    """Demostrar la integraciÃ³n de vLLM con ModelSync"""
    print("ğŸš€ ModelSync vLLM Integration Demo")
    print("=" * 50)
    
    # Inicializar cliente
    client = VLLMClient()
    
    # 1. Verificar estado del servicio
    print("\n1ï¸âƒ£ Verificando estado del servicio...")
    try:
        health = client.health_check()
        print(f"âœ… Estado del servicio: {health['status']}")
        print(f"ğŸ“Š Modelos cargados: {health['models_loaded']}")
        print(f"ğŸ”— ModelSync: {health['modelsync_status']}")
    except Exception as e:
        print(f"âŒ Error conectando al servicio: {e}")
        print("ğŸ’¡ AsegÃºrate de que el servicio estÃ© ejecutÃ¡ndose:")
        print("   modelsync llm start")
        return
    
    # 2. Inicializar ModelSync si es necesario
    print("\n2ï¸âƒ£ Configurando ModelSync...")
    modelsync_status = client.get_modelsync_status()
    if modelsync_status["status"] == "not_initialized":
        print("ğŸ”§ Inicializando repositorio ModelSync...")
        init_result = client.init_modelsync()
        print(f"âœ… {init_result['message']}")
    else:
        print("âœ… ModelSync ya estÃ¡ inicializado")
    
    # 3. Listar modelos disponibles
    print("\n3ï¸âƒ£ Modelos disponibles:")
    models = client.list_models()
    for model in models:
        print(f"  â€¢ {model['name']} ({model['status']})")
        if model.get('version_control_id'):
            print(f"    ğŸ“ VersiÃ³n en ModelSync: {model['version_control_id']}")
    
    # 4. Generar texto simple
    print("\n4ï¸âƒ£ GeneraciÃ³n de texto simple...")
    try:
        response = client.generate(
            prompt="Hola, Â¿cÃ³mo estÃ¡s? CuÃ©ntame algo interesante sobre la inteligencia artificial.",
            max_tokens=100,
            temperature=0.7,
            save_to_version_control=True
        )
        
        print(f"ğŸ¤– Respuesta generada:")
        print(f"   {response['text']}")
        print(f"ğŸ“Š Uso de tokens: {response['usage']}")
        if response.get('model_version_id'):
            print(f"ğŸ’¾ Guardado en ModelSync: {response['model_version_id']}")
    except Exception as e:
        print(f"âŒ Error en generaciÃ³n: {e}")
    
    # 5. GeneraciÃ³n en lote
    print("\n5ï¸âƒ£ GeneraciÃ³n en lote...")
    prompts = [
        "Explica quÃ© es el machine learning en una frase",
        "Â¿CuÃ¡l es la diferencia entre AI y ML?",
        "Dame un consejo para programadores"
    ]
    
    try:
        batch_responses = client.generate_batch(
            prompts=prompts,
            max_tokens=50,
            temperature=0.8,
            save_to_version_control=True
        )
        
        print(f"ğŸ“¦ Generadas {len(batch_responses)} respuestas:")
        for i, response in enumerate(batch_responses):
            print(f"  {i+1}. {response['text'][:100]}...")
            if response.get('model_version_id'):
                print(f"     ğŸ’¾ ModelSync ID: {response['model_version_id']}")
    except Exception as e:
        print(f"âŒ Error en generaciÃ³n por lotes: {e}")
    
    # 6. Experimento avanzado
    print("\n6ï¸âƒ£ Ejecutando experimento avanzado...")
    experiment_manager = VLLMExperimentManager(client)
    
    try:
        experiment_result = experiment_manager.run_experiment(
            experiment_name="temperature_comparison",
            prompts=[
                "Escribe un poema sobre la tecnologÃ­a",
                "Explica el concepto de recursiÃ³n",
                "Â¿QuÃ© es la programaciÃ³n orientada a objetos?"
            ],
            parameters={
                "max_tokens": 80,
                "temperature": 0.9,
                "top_p": 0.95
            },
            description="Experimento comparando diferentes temperaturas de generaciÃ³n"
        )
        
        print(f"ğŸ§ª Experimento completado:")
        print(f"   Prompts procesados: {experiment_result['total_prompts']}")
        print(f"   Generaciones exitosas: {experiment_result['successful_generations']}")
        print(f"   Tokens totales: {experiment_result['total_tokens']}")
        
        # Mostrar algunas respuestas
        print(f"\nğŸ“ Algunas respuestas del experimento:")
        for i, result in enumerate(experiment_result['results'][:2]):
            if 'error' not in result:
                print(f"  {i+1}. {result['response'][:150]}...")
    except Exception as e:
        print(f"âŒ Error en experimento: {e}")
    
    # 7. ComparaciÃ³n de parÃ¡metros
    print("\n7ï¸âƒ£ ComparaciÃ³n de parÃ¡metros...")
    try:
        comparison_result = experiment_manager.compare_parameters(
            base_prompts=["Escribe una historia corta sobre un robot"],
            parameter_sets=[
                {"temperature": 0.3, "max_tokens": 50},  # MÃ¡s conservador
                {"temperature": 0.7, "max_tokens": 50},  # Balanceado
                {"temperature": 1.2, "max_tokens": 50}   # MÃ¡s creativo
            ],
            experiment_name="temperature_comparison"
        )
        
        print(f"ğŸ”¬ ComparaciÃ³n completada:")
        print(f"   Conjuntos de parÃ¡metros: {len(comparison_result['parameter_sets'])}")
        print(f"   Prompts base: {len(comparison_result['base_prompts'])}")
        
        for i, result in enumerate(comparison_result['results']):
            param_set = result['parameter_set']
            experiment = result['experiment_result']
            print(f"  ParÃ¡metro {i+1} (temp={param_set['temperature']}):")
            print(f"    Exitosos: {experiment['successful_generations']}/{experiment['total_prompts']}")
            print(f"    Tokens: {experiment['total_tokens']}")
    except Exception as e:
        print(f"âŒ Error en comparaciÃ³n: {e}")
    
    # 8. MÃ©tricas finales
    print("\n8ï¸âƒ£ MÃ©tricas del servicio...")
    try:
        metrics = client.get_metrics()
        print(f"ğŸ“Š MÃ©tricas:")
        print(f"   Modelos cargados: {metrics['models_loaded']}")
        print(f"   Total de requests: {metrics['total_requests']}")
        print(f"   Actividad reciente: {metrics['recent_activity']}")
        print(f"   IntegraciÃ³n ModelSync: {metrics['modelsync_integration']}")
    except Exception as e:
        print(f"âŒ Error obteniendo mÃ©tricas: {e}")
    
    print("\nğŸ‰ Demo de vLLM con ModelSync completado!")
    print("\nğŸ“š Lo que se demostrÃ³:")
    print("   âœ… IntegraciÃ³n completa con ModelSync")
    print("   âœ… GeneraciÃ³n de texto individual y en lote")
    print("   âœ… Versionado automÃ¡tico de generaciones")
    print("   âœ… Experimentos y comparaciones de parÃ¡metros")
    print("   âœ… AuditorÃ­a y mÃ©tricas")
    print("   âœ… GestiÃ³n de modelos LLM")
    
    print("\nğŸš€ PrÃ³ximos pasos:")
    print("   â€¢ Usar CLI: modelsync llm generate --prompt 'tu prompt aquÃ­'")
    print("   â€¢ Ver estado: modelsync llm_status")
    print("   â€¢ Iniciar servicio: modelsync llm start")

if __name__ == "__main__":
    demonstrate_vllm_integration()
